# -*- coding: utf-8 -*-
"""cnnCyclicGAN_mnist

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yFI3Wuza0bCh_SQr45OwyDioal8WnpwY
"""

#https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-an-mnist-handwritten-digits-from-scratch-in-keras/
#https://theailearner.com/tag/cyclegan/
# example of training a gan on mnist
from numpy import expand_dims
from numpy import zeros
from numpy import ones
from numpy import vstack
import numpy as np
from numpy.random import randn
from numpy.random import randint
from keras.datasets.mnist import load_data
from keras.optimizers import Adam
from keras.models import Sequential
from keras.models import Model
from keras.models import load_model
from keras.layers import Input
from keras.layers import Dense
from keras.layers import Reshape
from keras.layers import Flatten
from keras.layers import Conv2D
from keras.layers import Conv2DTranspose
from keras.layers import LeakyReLU
from keras.layers import Dropout
from matplotlib import pyplot
from shutil import copyfile
from netCDF4 import Dataset

# define the standalone discriminator model
def define_discriminator(in_shape=(28,28,1)):
    model = Sequential()
    model.add(Conv2D(64, (3,3), strides=(2, 2), padding='same', input_shape=in_shape))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dropout(0.4))
    model.add(Conv2D(64, (3,3), strides=(2, 2), padding='same'))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dropout(0.4))
    model.add(Flatten())
    model.add(Dense(1, activation='sigmoid'))
    # compile model
    opt = Adam(lr=0.00002, beta_1=0.5)
    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])
    return model

# define the standalone generator model
def define_generator(in_shape=(28,28,1)):
    model = Sequential()
    model.add(Conv2D(64, (3,3), strides=(2, 2), padding='same', input_shape=in_shape))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dropout(0.4))
    model.add(Conv2D(64, (3,3), strides=(2, 2), padding='same'))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dropout(0.4))
    # model.add(Flatten())
    # n_nodes = 128 * 7 * 7
    # model.add(Dense(n_nodes))
    #model.add(LeakyReLU(alpha=0.2))
    #model.add(Reshape((7, 7, 128)))
    # upsample to 14x14
    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))
    model.add(LeakyReLU(alpha=0.2))
    # upsample to 28x28
    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Conv2D(1, (7,7), activation='sigmoid', padding='same'))
    return model

# define the combined generator and discriminator model, for updating the generator
def define_gan(g_model, d_model):
    # make weights in the discriminator not trainable
    d_model.trainable = False
    # connect them
    model = Sequential()
    # add generator
    model.add(g_model)
    # add the discriminator
    model.add(d_model)
    # compile model
    opt = Adam(lr=0.0002, beta_1=0.5)
    model.compile(loss='binary_crossentropy', optimizer=opt)
    return model

def define_combined(genA2B, genB2A, discA, discB, in_shape=(28,28,1)):
    inputA = Input(shape = in_shape) 
    inputB = Input(shape = in_shape) 
    gen_imgB = genA2B(inputA)
    gen_imgA = genB2A(inputB)
    
    #for cycle consistency
    reconstruct_imgA = genB2A(gen_imgB)
    reconstruct_imgB = genA2B(gen_imgA)
    
    # identity mapping
    gen_orig_imgB = genA2B(inputB)
    gen_orig_imgA = genB2A(inputA)
      
    discA.trainable = False
    discB.trainable = False
    
    valid_imgA = discA(gen_imgA)
    valid_imgB = discB(gen_imgB)
    
    opt = Adam(lr=0.0001, beta_1=0.5)
    comb_model = Model([inputA, inputB], [valid_imgA, valid_imgB, reconstruct_imgA, reconstruct_imgB, gen_orig_imgA, gen_orig_imgB])
#     comb_model.summary()
    comb_model.compile(loss=['binary_crossentropy', 'binary_crossentropy', 'mae', 'mae', 'mae','mae'],loss_weights=[  1, 1, 10, 10, 1, 1],optimizer=opt)
    return comb_model

# load and prepare mnist training images
def load_A_samples():
    # load mnist dataset
    dataset = Dataset("tas_day_IPSL-CM5A-MR_piControl_r1i1p1_18000101-18191231_coarsegrain_europe.nc", "r", format="NETCDF4")
    X = dataset.variables["tas"]
    # expand to 3d, e.g. add channels dimension
    X = expand_dims(X, axis=-1)
    # convert from unsigned ints to floats
    X = X.astype('float32')
    # scale from [0,255] to [0,1]
    X = (X - X.min()) / (X.max() - X.min()) 
    return X
def load_B_samples():
    # load mnist dataset
    dataset = Dataset("tas_day_IPSL-CM5A-MR_piControl_r1i1p1_18000101-18191231_europe.nc", "r", format="NETCDF4")
    X = dataset.variables["tas"]
    # expand to 3d, e.g. add channels dimension
    X = expand_dims(X, axis=-1)
    # convert from unsigned ints to floats
    X = X.astype('float32')
    # scale from [0,255] to [0,1]
    X = (X - X.min()) / (X.max() - X.min()) 
    return X

# select real samples
def generate_real_samples(dataset, n_samples):
    # choose random instances
    ix = randint(0, dataset.shape[0], n_samples)
    # retrieve selected images
    X = dataset[ix]
    # generate 'real' class labels (1)
    y = ones((n_samples, 1))
    return X, y

# use the generator to generate n fake examples, with class labels
def generate_fake_samples(g_model, x_input):
    # predict outputs
    X = g_model.predict(x_input)
    # create 'fake' class labels (0)
    y = zeros((np.size(x_input, 0), 1))
    return X, y

# create and save a plot of generated images (reversed grayscale)
def save_plot_gan(gen, x_input, epoch, n=10):
    # plot images
    x_input = x_input[range(n)]
    print(x_input.shape)

    x_fake, y_fake = generate_fake_samples(gen, x_input)
    print(x_fake.shape)

    examples = vstack((x_input, x_fake))
    print(examples.shape)
    for i in range(2 * n):
        # define subplot
            pyplot.subplot(2, n, 1 + i)
            # turn off axis
            pyplot.axis('off')
            # plot raw pixel data
            pyplot.imshow(examples[i, :, :, 0], cmap='gray_r')
    # save plot to file
    filename = 'generated_plot_e%03d.png' % (epoch+1)
    pyplot.savefig(filename, dpi=150)
    pyplot.close()

# create and save a plot of generated images (reversed grayscale)
def save_plot_combined(xA_real, genA2B, genB2A, epoch, n=10):
    # plot images
    xA_real = xA_real[range(n)]
    xA2A, _ = generate_fake_samples(genB2A, xA_real)
    xA2B, _ = generate_fake_samples(genA2B, xA_real)
    xA2B2A, _ = generate_fake_samples(genB2A, xA2B)
    examples = vstack((xA_real, xA2A, xA2B, xA2B2A))
    print(examples.shape)
    for i in range(4 * n):
        # define subplot
        pyplot.subplot(4, n, 1 + i)
        # turn off axis
        pyplot.axis('off')
        # plot raw pixel data
        pyplot.imshow(examples[i, :, :, 0], cmap='gray_r')
    # save plot to file
    filename = 'generated_plot_e%03d.png' % (epoch+1)
    pyplot.savefig(filename, dpi=150)
    pyplot.close()


# evaluate the discriminator, plot generated images, save generator model
def summarize_performance_gan(epoch, g_model, d_model, dataset, inputset, n_samples=100):
    savepath = './'
    # prepare real samples
    X_real, y_real = generate_real_samples(dataset, n_samples)
    # evaluate discriminator on real examples
    _, acc_real = d_model.evaluate(X_real, y_real, verbose=0)
    # prepare fake examples
    x_input = inputset[randint(0, inputset.shape[0], n_samples)]
    x_fake, y_fake = generate_fake_samples(g_model, x_input)
    # evaluate discriminator on fake examples
    _, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)
    # summarize discriminator performance
    print('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))
    # save plot
    save_plot_gan(g_model, x_input, epoch)
    # save the generator model tile file
    filename = savepath + 'generator_model.h5' 
    g_model.save(filename)
    filename = savepath + 'discriminator_model.h5'
    d_model.save(filename)

# evaluate the discriminator, plot generated images, save generator model
def summarize_performance_combined(epoch, genA2B, genB2A, discA, discB, datasetA, datasetB, n_samples=100):
    savepath = './'
    # prepare real samples
    xA_real, yA_real = generate_real_samples(datasetA, n_samples)
    xB_real, yB_real = generate_real_samples(datasetB, n_samples)
    # evaluate discriminator on real examples
    _, accA_real = discA.evaluate(xA_real, yA_real, verbose=0)
    _, accB_real = discB.evaluate(xB_real, yB_real, verbose=0)
    # prepare fake examples
    xA_fake, yA_fake = generate_fake_samples(genB2A, xB_real)
    xB_fake, yB_fake = generate_fake_samples(genA2B, xA_real)
    # evaluate discriminator on fake examples
    _, accA_fake = discA.evaluate(xA_fake, yA_fake, verbose=0)
    _, accB_fake = discB.evaluate(xB_fake, yB_fake, verbose=0)
    # summarize discriminator performance
    print('>Accuracy A real: %.0f%%, A fake: %.0f%%' % (accA_real*100, accA_fake*100))
    print('>Accuracy B real: %.0f%%, B fake: %.0f%%' % (accB_real*100, accB_fake*100))
    # save plot
    save_plot_combined(xA_real, genA2B, genB2A, epoch)
    # save the generator model tile file
    genA2B.save(savepath + 'genA2B.h5')
    genB2A.save(savepath + 'genB2A.h5')
    # save the discriminator model tile file
    discA.save(savepath + 'discA.h5')
    discB.save(savepath + 'discB.h5')

def train_gan(gen, disc, gan, dataset, inputset, n_epochs=100, n_batch=256):
    bat_per_epo = int(dataset.shape[0] / n_batch)
    half_batch = int(n_batch / 2)
    # manually enumerate epochs
    for i in range(n_epochs):
        # enumerate batches over the training set
        for j in range(bat_per_epo):
            # get randomly selected 'real' samples
            x_real, y_real = generate_real_samples(dataset, half_batch)
            # generate 'fake' examples
            x_input = inputset[randint(0, inputset.shape[0], half_batch)]
            x_fake, y_fake = generate_fake_samples(gen, x_input)
            # create training set for the discriminator
            x, y = vstack((x_real, x_fake)), vstack((y_real, y_fake))
            # update discriminator model weights
            d_loss, _ = disc.train_on_batch(x, y)
            # train generator
            x_gan, y_gan = generate_real_samples(inputset, n_batch)
            g_loss = gan.train_on_batch(x_gan, y_gan)
            # evaluate the model performance, sometimes
        print('>%d, %d/%d, d=%.3f, g=%.3f' % (i+1, j+1, bat_per_epo, d_loss, g_loss))
        if (i+1) % 50 == 0:
            summarize_performance_gan(i, gen, disc, dataset, inputset)



def train_combined(genA2B, genB2A, discA, discB, comb_model, datasetA, datasetB, n_epochs=100, n_batch=32):
    bat_per_epo = int(datasetA.shape[0] / n_batch)
    half_batch = int(n_batch / 2)
    # manually enumerate epochs
    for i in range(n_epochs):
        # enumerate batches over the training set
        for j in range(bat_per_epo):
            # get randomly selected 'real' samples
            xA_real, yA_real = generate_real_samples(datasetA, half_batch)
            xB_real, yB_real = generate_real_samples(datasetB, half_batch)
            # generate 'fake' examples
            xA_fake, yA_fake = generate_fake_samples(genB2A, xB_real)
            xB_fake, yB_fake = generate_fake_samples(genA2B, xA_real)
            # create training set for the discriminator
            xA, yA = vstack((xA_real, xA_fake)), vstack((yA_real, yA_fake))
            xB, yB = vstack((xB_real, xB_fake)), vstack((yB_real, yB_fake))
            # update discriminator model weights
            dA_loss, _ = discA.train_on_batch(xA, yA)
            dB_loss, _ = discB.train_on_batch(xB, yB)
            # train generator
            g_loss = comb_model.train_on_batch([xA_real, xB_real], [yB_real, yA_real, xA_real, xB_real, xA_real, xB_real])
            # evaluate the model performance, sometimes
        # summarize loss on this batch
        pyplot.subplot(1, 2, 1)
        pyplot.axis('off')
        pyplot.imshow(xA_fake[0,:,:,0], cmap='gray_r')
        pyplot.subplot(1, 2, 2)
        pyplot.axis('off')
        pyplot.imshow(xB_fake[0,:,:,0], cmap='gray_r')
        pyplot.savefig('sampleAB.png', dpi=150)
        pyplot.close()
        print('>%d, %d/%d, dA=%.3f, dB=%.3f, gB2A=%.3f, gA2B=%.3f, g=%.3f' % (i+1, j+1, bat_per_epo, dA_loss, dB_loss, g_loss[0], g_loss[1], sum(g_loss)))
        if (i+1) % 10 == 1:
            summarize_performance_combined(i, genA2B, genB2A, discA, discB, datasetA, datasetB)

# create the discriminator
discA = define_discriminator()
discB = define_discriminator()
# create the generator
genA2B = define_generator()
genB2A = define_generator()
# load weights
#savepath = './'
#discA = load_model(savepath + 'discA.h5')
#discB = load_model(savepath + 'discB.h5')
#genA2B = load_model(savepath + 'genA2B.h5')
#genB2A = load_model(savepath + 'genB2A.h5')
# create the generator
#ganA2B = define_gan(genA2B, discB)
#ganB2A = define_gan(genB2A, discA)
# create the gan
comb_model = define_combined(genA2B, genB2A, discA, discB)
# load image data
datasetA = load_A_samples()
datasetB = load_B_samples()
nimage = min(datasetA.shape[0], datasetB.shape[0])
datasetA = datasetA[range(nimage)]
datasetB = datasetB[range(nimage)]
print(datasetA.shape)
# train model
# train_gan(genA2B, discB, ganA2B, datasetB, inputset = datasetA, n_epochs = 1000)
# copyfile(savepath + 'generator_model.h5', savepath + 'genA2B.h5')
# copyfile(savepath + 'discriminator_model.h5', savepath + 'discB.h5')
# train_gan(genB2A, discA, ganB2A, datasetA, inputset = datasetB, n_epochs = 1000)
# copyfile(savepath + 'generator_model.h5', savepath + 'genB2A.h5')
# copyfile(savepath + 'discriminator_model.h5', savepath + 'discA.h5')
train_combined(genA2B, genB2A, discA, discB, comb_model, datasetA, datasetB, n_epochs = 100)

# Validation with real pairs
fakesetB = genA2B(datasetA)
nchecks = 10
examples = vstack((datasetA[range(nchecks)], fakesetB[range(nchecks)]))
print(examples.shape)
for i in range(2 * n):
    # define subplot
    pyplot.subplot(2, n, 1 + i)
    # turn off axis
    pyplot.axis('off')
    # plot raw pixel data
    pyplot.imshow(examples[i, :, :, 0], cmap='gray_r')
    # save plot to file
    filename = 'Validation.png' % (epoch+1)
pyplot.savefig(filename, dpi=150)
pyplot.close()
